{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>TODO</p>"},{"location":"#motivation","title":"Motivation","text":"<p>TODO</p>"},{"location":"#api-stability","title":"API stability","text":"<p> While <code>sonnix</code> is in development stage, no API is guaranteed to be stable from one release to the next. In fact, it is very likely that the API will change multiple times before a stable 1.0.0 release. In practice, this means that upgrading <code>sonnix</code> to a new version will possibly break any code that was using the old version of <code>sonnix</code>.</p>"},{"location":"#license","title":"License","text":"<p><code>sonnix</code> is licensed under BSD 3-Clause \"New\" or \"Revised\" license available in LICENSE file.</p>"},{"location":"get_started/","title":"Get Started","text":"<p>It is highly recommended to install in a virtual environment to keep your system in order.</p>"},{"location":"get_started/#installing-with-pip-recommended","title":"Installing with <code>pip</code> (recommended)","text":"<p>The following command installs the latest version of the library:</p> <pre><code>pip install sonnix\n</code></pre> <p>To make the package as slim as possible, only the packages required to use <code>sonnix</code> are installed. It is possible to install all the optional dependencies by running the following command:</p> <pre><code>pip install 'sonnix[all]'\n</code></pre> <p>This command also installed NumPy and PyTorch. It is also possible to install the optional packages manually or to select the packages to install. In the following example, only NumPy is installed:</p> <pre><code>pip install sonnix numpy\n</code></pre>"},{"location":"get_started/#installing-from-source","title":"Installing from source","text":"<p>To install <code>sonnix</code> from source, you can follow the steps below. First, you will need to install <code>poetry</code>. <code>poetry</code> is used to manage and install the dependencies. If <code>poetry</code> is already installed on your machine, you can skip this step. There are several ways to install <code>poetry</code> so you can use the one that you prefer. You can check the <code>poetry</code> installation by running the following command:</p> <pre><code>poetry --version\n</code></pre> <p>Then, you can clone the git repository:</p> <pre><code>git clone git@github.com:durandtibo/sonnix.git\n</code></pre> <p>It is recommended to create a Python 3.8+ virtual environment. This step is optional so you can skip it. To create a virtual environment, you can use the following command:</p> <pre><code>make conda\n</code></pre> <p>It automatically creates a conda virtual environment. When the virtual environment is created, you can activate it with the following command:</p> <pre><code>conda activate sonnix\n</code></pre> <p>This example uses <code>conda</code> to create a virtual environment, but you can use other tools or configurations. Then, you should install the required package to use <code>sonnix</code> with the following command:</p> <pre><code>make install\n</code></pre> <p>This command will install all the required packages. You can also use this command to update the required packages. This command will check if there is a more recent package available and will install it. Finally, you can test the installation with the following command:</p> <pre><code>make unit-test-cov\n</code></pre>"},{"location":"refs/functional/","title":"sonnix.functional","text":""},{"location":"refs/functional/#activations","title":"Activations","text":""},{"location":"refs/functional/#sonnix.functional.safe_exp","title":"sonnix.functional.safe_exp","text":"<pre><code>safe_exp(input: Tensor, max: float = 20.0) -&gt; Tensor\n</code></pre> <p>Compute safely the exponential of the elements.</p> <p>The values that are higher than the specified minimum value are set to this maximum value. Using a not too large positive value leads to an output tensor without Inf.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>max</code> <code>float</code> <p>The maximum value.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor with the exponential of the elements.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import safe_exp\n&gt;&gt;&gt; output = safe_exp(torch.tensor([1.0, 10.0, 100.0, 1000.0]))\n&gt;&gt;&gt; output\ntensor([2.7183e+00, 2.2026e+04, 4.8517e+08, 4.8517e+08])\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.safe_log","title":"sonnix.functional.safe_log","text":"<pre><code>safe_log(input: Tensor, min: float = 1e-08) -&gt; Tensor\n</code></pre> <p>Compute safely the logarithm natural logarithm of the elements.</p> <p>The values that are lower than the specified minimum value are set to this minimum value. Using a small positive value leads to an output tensor without NaN or Inf.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>min</code> <code>float</code> <p>The minimum value.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor with the natural logarithm of the elements.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import safe_log\n&gt;&gt;&gt; safe_log(torch.tensor([1e-4, 1e-5, 1e-6, 1e-8, 1e-9, 1e-10]))\ntensor([ -9.2103, -11.5129, -13.8155, -18.4207, -18.4207, -18.4207])\n</code></pre>"},{"location":"refs/functional/#loss-functions","title":"Loss functions","text":""},{"location":"refs/functional/#sonnix.functional.asinh_mse_loss","title":"sonnix.functional.asinh_mse_loss","text":"<pre><code>asinh_mse_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the mean squared error (MSE) on the inverse hyperbolic sine (asinh) transformed predictions and targets.</p> <p>It is a generalization of mean squared logarithmic error (MSLE) that works for real values. The <code>asinh</code> transformation is used instead of <code>log1p</code> because <code>asinh</code> works on negative values.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean squared error (MSE) on the inverse hyperbolic sine (asinh) transformed predictions and targets. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import asinh_mse_loss\n&gt;&gt;&gt; loss = asinh_mse_loss(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MseLossBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.asinh_smooth_l1_loss","title":"sonnix.functional.asinh_smooth_l1_loss","text":"<pre><code>asinh_smooth_l1_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n    beta: float = 1.0,\n) -&gt; Tensor\n</code></pre> <p>Compute the smooth L1 loss on the inverse hyperbolic sine (asinh) transformed predictions and targets.</p> <p>It is a generalization of mean squared logarithmic error (MSLE) that works for real values. The <code>asinh</code> transformation is used instead of <code>log1p</code> because <code>asinh</code> works on negative values.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>beta</code> <code>float</code> <p>The threshold at which to change between L1 and L2 loss. The value must be non-negative.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The smooth L1 loss on the inverse hyperbolic sine (asinh) transformed predictions and targets. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import asinh_smooth_l1_loss\n&gt;&gt;&gt; loss = asinh_smooth_l1_loss(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;SmoothL1LossBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.binary_focal_loss","title":"sonnix.functional.binary_focal_loss","text":"<pre><code>binary_focal_loss(\n    prediction: Tensor,\n    target: Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2.0,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the binary focal loss.</p> <p>Based on \"Focal Loss for Dense Object Detection\" (https://arxiv.org/pdf/1708.02002.pdf) Implementation is based on https://pytorch.org/vision/main/_modules/torchvision/ops/focal_loss.html</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The float tensor with predictions as probabilities for each example.</p> required <code>target</code> <code>Tensor</code> <p>A float tensor with the same shape as inputs. It stores the binary classification label for each element in inputs (0 for the negative class and 1 for the positive class).</p> required <code>alpha</code> <code>float</code> <p>The weighting factor, which must be in the range <code>[0, 1]</code>. This parameter is ignored if negative.</p> <code>0.25</code> <code>gamma</code> <code>float</code> <p>The focusing parameter, which must be positive (<code>&gt;=0</code>).</p> <code>2.0</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed binary focal loss. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import binary_focal_loss\n&gt;&gt;&gt; loss = binary_focal_loss(\n...     torch.rand(2, 4, requires_grad=True),\n...     torch.tensor([[1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0]]),\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.binary_focal_loss_with_logits","title":"sonnix.functional.binary_focal_loss_with_logits","text":"<pre><code>binary_focal_loss_with_logits(\n    prediction: Tensor,\n    target: Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2.0,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the binary focal loss.</p> <p>Based on \"Focal Loss for Dense Object Detection\" (https://arxiv.org/pdf/1708.02002.pdf) Implementation is based on https://pytorch.org/vision/main/_modules/torchvision/ops/focal_loss.html</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The float tensor with predictions as unnormalized scores (often referred to as logits) for each example.</p> required <code>target</code> <code>Tensor</code> <p>A float tensor with the same shape as inputs. It stores the binary classification label for each element in inputs (0 for the negative class and 1 for the positive class).</p> required <code>alpha</code> <code>float</code> <p>The weighting factor, which must be in the range <code>[0, 1]</code>. This parameter is ignored if negative.</p> <code>0.25</code> <code>gamma</code> <code>float</code> <p>The focusing parameter, which must be positive (<code>&gt;=0</code>).</p> <code>2.0</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed binary focal loss. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import binary_focal_loss_with_logits\n&gt;&gt;&gt; loss = binary_focal_loss_with_logits(\n...     torch.randn(2, 4, requires_grad=True),\n...     torch.tensor([[1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0]]),\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.binary_poly1_loss","title":"sonnix.functional.binary_poly1_loss","text":"<pre><code>binary_poly1_loss(\n    prediction: Tensor,\n    target: Tensor,\n    alpha: float = 1.0,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the Poly-1 loss for binary targets.</p> <p>Based on \"PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions\" (https://arxiv.org/pdf/2204.12511)</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The float tensor with predictions as probabilities for each example.</p> required <code>target</code> <code>Tensor</code> <p>A float tensor with the same shape as inputs. It stores the binary classification label for each element in inputs (0 for the negative class and 1 for the positive class).</p> required <code>alpha</code> <code>float</code> <p>The weighting factor, which must be in the range <code>[0, 1]</code>. This parameter is ignored if negative.</p> <code>1.0</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed Poly-1 loss value. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import binary_poly1_loss\n&gt;&gt;&gt; loss = binary_poly1_loss(\n...     torch.rand(2, 4, requires_grad=True),\n...     torch.tensor([[1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0]]),\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.binary_poly1_loss_with_logits","title":"sonnix.functional.binary_poly1_loss_with_logits","text":"<pre><code>binary_poly1_loss_with_logits(\n    prediction: Tensor,\n    target: Tensor,\n    alpha: float = 1.0,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the Poly-1 loss.</p> <p>Based on \"PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions\" (https://arxiv.org/pdf/2204.12511)</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The float tensor with predictions as unnormalized scores (often referred to as logits) for each example.</p> required <code>target</code> <code>Tensor</code> <p>A float tensor with the same shape as inputs. It stores the binary classification label for each element in inputs (0 for the negative class and 1 for the positive class).</p> required <code>alpha</code> <code>float</code> <p>The weighting factor, which must be in the range <code>[0, 1]</code>. This parameter is ignored if negative.</p> <code>1.0</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed Poly-1 loss value. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import binary_poly1_loss_with_logits\n&gt;&gt;&gt; loss = binary_poly1_loss_with_logits(\n...     torch.randn(2, 4, requires_grad=True),\n...     torch.tensor([[1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0]]),\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.general_robust_regression_loss","title":"sonnix.functional.general_robust_regression_loss","text":"<pre><code>general_robust_regression_loss(\n    prediction: Tensor,\n    target: Tensor,\n    alpha: float = 2.0,\n    scale: float = 1.0,\n    max: float | None = None,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the general robust regression loss a.k.a. Barron robust loss.</p> <p>Based on the paper:</p> <pre><code>A General and Adaptive Robust Loss Function\nJonathan T. Barron\nCVPR 2019 (https://arxiv.org/abs/1701.03077)\n</code></pre> Note <p>The \"adaptative\" part of the loss is not implemented in this     function.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>alpha</code> <code>float</code> <p>The shape parameter that controls the robustness of the loss.</p> <code>2.0</code> <code>scale</code> <code>float</code> <p>The scale parameter that controls the size of the loss's quadratic bowl near 0.</p> <code>1.0</code> <code>max</code> <code>float | None</code> <p>The max value to clip the loss before to compute the reduction. <code>None</code> means no clipping is used.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The loss. The shape of the tensor depends on the reduction strategy.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import general_robust_regression_loss\n&gt;&gt;&gt; loss = general_robust_regression_loss(\n...     torch.randn(2, 4, requires_grad=True), torch.randn(2, 4)\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.log_cosh_loss","title":"sonnix.functional.log_cosh_loss","text":"<pre><code>log_cosh_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n    scale: float = 1.0,\n) -&gt; Tensor\n</code></pre> <p>Compute the logarithm of the hyperbolic cosine of the prediction error.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>scale</code> <code>float</code> <p>The scale factor.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The logarithm of the hyperbolic cosine of the prediction error.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import log_cosh_loss\n&gt;&gt;&gt; loss = log_cosh_loss(torch.randn(3, 5, requires_grad=True), torch.randn(3, 5))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.msle_loss","title":"sonnix.functional.msle_loss","text":"<pre><code>msle_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the mean squared error (MSE) on the logarithmic transformed predictions and targets.</p> <p>This loss is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this loss penalizes an under-predicted estimate greater than an over-predicted estimate.</p> <p>Note: this loss only works with positive values (0 included).</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean squared logarithmic error. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import msle_loss\n&gt;&gt;&gt; loss = msle_loss(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MseLossBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.poisson_regression_loss","title":"sonnix.functional.poisson_regression_loss","text":"<pre><code>poisson_regression_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n    eps: float = 1e-08,\n) -&gt; Tensor\n</code></pre> <p>Compute the Poisson regression loss.</p> <p>Loss Functions and Metrics in Deep Learning https://arxiv.org/pdf/2307.02694</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The count predictions. The values must be positive.</p> required <code>target</code> <code>Tensor</code> <p>The count target values. The values must be positive.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the count is zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The Poisson regression loss. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import poisson_regression_loss\n&gt;&gt;&gt; loss = poisson_regression_loss(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.quantile_regression_loss","title":"sonnix.functional.quantile_regression_loss","text":"<pre><code>quantile_regression_loss(\n    prediction: Tensor,\n    target: Tensor,\n    q: float = 0.5,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the quantile regression loss.</p> <p>Loss Functions and Metrics in Deep Learning https://arxiv.org/pdf/2307.02694</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>q</code> <code>float</code> <p>The quantile value. <code>q=0.5</code> is equivalent to the Mean Absolute Error (MAE).</p> <code>0.5</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The quantile regression loss. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import quantile_regression_loss\n&gt;&gt;&gt; loss = quantile_regression_loss(\n...     torch.randn(2, 4, requires_grad=True), torch.randn(2, 4)\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.reduce_loss","title":"sonnix.functional.reduce_loss","text":"<pre><code>reduce_loss(tensor: Tensor, reduction: str) -&gt; Tensor\n</code></pre> <p>Return the reduced loss.</p> <p>This function is designed to be used with loss functions.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor to reduce.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  <code>'sum'</code>, and <code>'batchmean'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed, <code>'batchmean'</code>: the sum will be divided by the size of the first dimension.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The reduced tensor. The shape of the tensor depends on the reduction strategy.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import reduce_loss\n&gt;&gt;&gt; tensor = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0]])\n&gt;&gt;&gt; reduce_loss(tensor, \"none\")\ntensor([[0., 1., 2.],\n        [3., 4., 5.]])\n&gt;&gt;&gt; reduce_loss(tensor, \"sum\")\ntensor(15.)\n&gt;&gt;&gt; reduce_loss(tensor, \"mean\")\ntensor(2.5000)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.relative_loss","title":"sonnix.functional.relative_loss","text":"<pre><code>relative_loss(\n    loss: Tensor,\n    indicator: Tensor,\n    reduction: str = \"mean\",\n    eps: float = 1e-08,\n) -&gt; Tensor\n</code></pre> <p>Compute the relative loss.</p> <p>The indicators are designed based on https://en.wikipedia.org/wiki/Relative_change#Indicators_of_relative_change.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Tensor</code> <p>The loss values. The tensor must have the same shape as the target.</p> required <code>indicator</code> <code>Tensor</code> <p>The indicator values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the indicator is zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed relative loss.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the loss and indicator shapes do not match.</p> <code>ValueError</code> <p>if the reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import relative_loss\n&gt;&gt;&gt; prediction = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; target = torch.randn(3, 5)\n&gt;&gt;&gt; loss = relative_loss(\n...     loss=torch.nn.functional.mse_loss(prediction, target, reduction=\"none\"),\n...     indicator=classical_relative_indicator(prediction, target),\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#relative-loss-indicators","title":"Relative loss indicators","text":""},{"location":"refs/functional/#sonnix.functional.loss.arithmetical_mean_indicator","title":"sonnix.functional.loss.arithmetical_mean_indicator","text":"<pre><code>arithmetical_mean_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the arithmetical mean change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import arithmetical_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = arithmetical_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[1.0000, 1.0000, 0.5000],\n        [3.0000, 3.0000, 1.0000]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.loss.classical_relative_indicator","title":"sonnix.functional.loss.classical_relative_indicator","text":"<pre><code>classical_relative_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the classical relative change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import classical_relative_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = classical_relative_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[2., 1., 0.],\n        [3., 5., 1.]])\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.loss.geometric_mean_indicator","title":"sonnix.functional.loss.geometric_mean_indicator","text":"<pre><code>geometric_mean_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the geometric mean change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import geometric_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = geometric_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[0.0000, 1.0000, 0.0000],\n        [3.0000, 2.2361, 1.0000]], grad_fn=&lt;SqrtBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.loss.maximum_mean_indicator","title":"sonnix.functional.loss.maximum_mean_indicator","text":"<pre><code>maximum_mean_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the maximum mean change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import maximum_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = maximum_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[2., 1., 1.],\n        [3., 5., 1.]], grad_fn=&lt;MaximumBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.loss.minimum_mean_indicator","title":"sonnix.functional.loss.minimum_mean_indicator","text":"<pre><code>minimum_mean_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the minimum mean change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import minimum_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = minimum_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[0., 1., 0.],\n        [3., 1., 1.]], grad_fn=&lt;MinimumBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.loss.moment_mean_indicator","title":"sonnix.functional.loss.moment_mean_indicator","text":"<pre><code>moment_mean_indicator(\n    prediction: Tensor, target: Tensor, k: int = 1\n) -&gt; Tensor\n</code></pre> <p>Return the moment mean change of order k.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>k</code> <code>int</code> <p>The order.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import moment_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = moment_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[1.0000, 1.0000, 0.5000],\n        [3.0000, 3.0000, 1.0000]], grad_fn=&lt;PowBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.loss.reversed_relative_indicator","title":"sonnix.functional.loss.reversed_relative_indicator","text":"<pre><code>reversed_relative_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the reversed relative change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import reversed_relative_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = reversed_relative_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[0., 1., 1.],\n        [3., 1., 1.]], grad_fn=&lt;AbsBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#errors","title":"Errors","text":""},{"location":"refs/functional/#sonnix.functional.absolute_error","title":"sonnix.functional.absolute_error","text":"<pre><code>absolute_error(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Compute the element-wise absolute error between the predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The tensor of predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor, which must have the same shape and data type as <code>prediction</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The absolute error tensor, which has the same shape and data type as the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import absolute_error\n&gt;&gt;&gt; absolute_error(torch.eye(2), torch.ones(2, 2))\ntensor([[0., 1.],\n        [1., 0.]])\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.absolute_relative_error","title":"sonnix.functional.absolute_relative_error","text":"<pre><code>absolute_relative_error(\n    prediction: Tensor, target: Tensor, eps: float = 1e-08\n) -&gt; Tensor\n</code></pre> <p>Compute the element-wise absolute relative error between the predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The tensor of predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor, which must have the same shape and data type as <code>prediction</code>.</p> required <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the target is zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The absolute relative error tensor, which has the same shape and data type as the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import absolute_relative_error\n&gt;&gt;&gt; absolute_relative_error(torch.eye(2), torch.ones(2, 2))\ntensor([[0., 1.],\n        [1., 0.]])\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.symmetric_absolute_relative_error","title":"sonnix.functional.symmetric_absolute_relative_error","text":"<pre><code>symmetric_absolute_relative_error(\n    prediction: Tensor, target: Tensor, eps: float = 1e-08\n) -&gt; Tensor\n</code></pre> <p>Compute the element-wise symmetric absolute relative error between the predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The tensor of predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor, which must have the same shape and data type as <code>prediction</code>.</p> required <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the target is zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The symmetric absolute relative error tensor, which has the same shape and data type as the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import symmetric_absolute_relative_error\n&gt;&gt;&gt; symmetric_absolute_relative_error(torch.eye(2), torch.ones(2, 2))\ntensor([[0., 2.],\n        [2., 0.]])\n</code></pre>"},{"location":"refs/functional/#utility","title":"Utility","text":""},{"location":"refs/functional/#sonnix.functional.check_loss_reduction_strategy","title":"sonnix.functional.check_loss_reduction_strategy","text":"<pre><code>check_loss_reduction_strategy(reduction: str) -&gt; None\n</code></pre> <p>Check if the provided reduction ia a valid loss reduction.</p> <p>The valid reduction values are <code>'mean'</code>, <code>'none'</code>,  <code>'sum'</code>, and <code>'batchmean'</code>.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>str</code> <p>The reduction strategy to check.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the provided reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from sonnix.functional import check_loss_reduction_strategy\n&gt;&gt;&gt; check_loss_reduction_strategy(\"mean\")\n</code></pre>"},{"location":"refs/functional/#all","title":"All","text":""},{"location":"refs/functional/#sonnix.functional","title":"sonnix.functional","text":"<p>Contain functional implementation of some modules.</p>"},{"location":"refs/functional/#sonnix.functional.absolute_error","title":"sonnix.functional.absolute_error","text":"<pre><code>absolute_error(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Compute the element-wise absolute error between the predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The tensor of predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor, which must have the same shape and data type as <code>prediction</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The absolute error tensor, which has the same shape and data type as the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import absolute_error\n&gt;&gt;&gt; absolute_error(torch.eye(2), torch.ones(2, 2))\ntensor([[0., 1.],\n        [1., 0.]])\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.absolute_relative_error","title":"sonnix.functional.absolute_relative_error","text":"<pre><code>absolute_relative_error(\n    prediction: Tensor, target: Tensor, eps: float = 1e-08\n) -&gt; Tensor\n</code></pre> <p>Compute the element-wise absolute relative error between the predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The tensor of predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor, which must have the same shape and data type as <code>prediction</code>.</p> required <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the target is zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The absolute relative error tensor, which has the same shape and data type as the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import absolute_relative_error\n&gt;&gt;&gt; absolute_relative_error(torch.eye(2), torch.ones(2, 2))\ntensor([[0., 1.],\n        [1., 0.]])\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.arithmetical_mean_indicator","title":"sonnix.functional.arithmetical_mean_indicator","text":"<pre><code>arithmetical_mean_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the arithmetical mean change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import arithmetical_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = arithmetical_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[1.0000, 1.0000, 0.5000],\n        [3.0000, 3.0000, 1.0000]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.asinh_mse_loss","title":"sonnix.functional.asinh_mse_loss","text":"<pre><code>asinh_mse_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the mean squared error (MSE) on the inverse hyperbolic sine (asinh) transformed predictions and targets.</p> <p>It is a generalization of mean squared logarithmic error (MSLE) that works for real values. The <code>asinh</code> transformation is used instead of <code>log1p</code> because <code>asinh</code> works on negative values.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean squared error (MSE) on the inverse hyperbolic sine (asinh) transformed predictions and targets. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import asinh_mse_loss\n&gt;&gt;&gt; loss = asinh_mse_loss(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MseLossBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.asinh_smooth_l1_loss","title":"sonnix.functional.asinh_smooth_l1_loss","text":"<pre><code>asinh_smooth_l1_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n    beta: float = 1.0,\n) -&gt; Tensor\n</code></pre> <p>Compute the smooth L1 loss on the inverse hyperbolic sine (asinh) transformed predictions and targets.</p> <p>It is a generalization of mean squared logarithmic error (MSLE) that works for real values. The <code>asinh</code> transformation is used instead of <code>log1p</code> because <code>asinh</code> works on negative values.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>beta</code> <code>float</code> <p>The threshold at which to change between L1 and L2 loss. The value must be non-negative.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The smooth L1 loss on the inverse hyperbolic sine (asinh) transformed predictions and targets. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import asinh_smooth_l1_loss\n&gt;&gt;&gt; loss = asinh_smooth_l1_loss(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;SmoothL1LossBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.binary_focal_loss","title":"sonnix.functional.binary_focal_loss","text":"<pre><code>binary_focal_loss(\n    prediction: Tensor,\n    target: Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2.0,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the binary focal loss.</p> <p>Based on \"Focal Loss for Dense Object Detection\" (https://arxiv.org/pdf/1708.02002.pdf) Implementation is based on https://pytorch.org/vision/main/_modules/torchvision/ops/focal_loss.html</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The float tensor with predictions as probabilities for each example.</p> required <code>target</code> <code>Tensor</code> <p>A float tensor with the same shape as inputs. It stores the binary classification label for each element in inputs (0 for the negative class and 1 for the positive class).</p> required <code>alpha</code> <code>float</code> <p>The weighting factor, which must be in the range <code>[0, 1]</code>. This parameter is ignored if negative.</p> <code>0.25</code> <code>gamma</code> <code>float</code> <p>The focusing parameter, which must be positive (<code>&gt;=0</code>).</p> <code>2.0</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed binary focal loss. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import binary_focal_loss\n&gt;&gt;&gt; loss = binary_focal_loss(\n...     torch.rand(2, 4, requires_grad=True),\n...     torch.tensor([[1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0]]),\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.binary_focal_loss_with_logits","title":"sonnix.functional.binary_focal_loss_with_logits","text":"<pre><code>binary_focal_loss_with_logits(\n    prediction: Tensor,\n    target: Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2.0,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the binary focal loss.</p> <p>Based on \"Focal Loss for Dense Object Detection\" (https://arxiv.org/pdf/1708.02002.pdf) Implementation is based on https://pytorch.org/vision/main/_modules/torchvision/ops/focal_loss.html</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The float tensor with predictions as unnormalized scores (often referred to as logits) for each example.</p> required <code>target</code> <code>Tensor</code> <p>A float tensor with the same shape as inputs. It stores the binary classification label for each element in inputs (0 for the negative class and 1 for the positive class).</p> required <code>alpha</code> <code>float</code> <p>The weighting factor, which must be in the range <code>[0, 1]</code>. This parameter is ignored if negative.</p> <code>0.25</code> <code>gamma</code> <code>float</code> <p>The focusing parameter, which must be positive (<code>&gt;=0</code>).</p> <code>2.0</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed binary focal loss. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import binary_focal_loss_with_logits\n&gt;&gt;&gt; loss = binary_focal_loss_with_logits(\n...     torch.randn(2, 4, requires_grad=True),\n...     torch.tensor([[1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0]]),\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.binary_poly1_loss","title":"sonnix.functional.binary_poly1_loss","text":"<pre><code>binary_poly1_loss(\n    prediction: Tensor,\n    target: Tensor,\n    alpha: float = 1.0,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the Poly-1 loss for binary targets.</p> <p>Based on \"PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions\" (https://arxiv.org/pdf/2204.12511)</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The float tensor with predictions as probabilities for each example.</p> required <code>target</code> <code>Tensor</code> <p>A float tensor with the same shape as inputs. It stores the binary classification label for each element in inputs (0 for the negative class and 1 for the positive class).</p> required <code>alpha</code> <code>float</code> <p>The weighting factor, which must be in the range <code>[0, 1]</code>. This parameter is ignored if negative.</p> <code>1.0</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed Poly-1 loss value. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import binary_poly1_loss\n&gt;&gt;&gt; loss = binary_poly1_loss(\n...     torch.rand(2, 4, requires_grad=True),\n...     torch.tensor([[1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0]]),\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.binary_poly1_loss_with_logits","title":"sonnix.functional.binary_poly1_loss_with_logits","text":"<pre><code>binary_poly1_loss_with_logits(\n    prediction: Tensor,\n    target: Tensor,\n    alpha: float = 1.0,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the Poly-1 loss.</p> <p>Based on \"PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions\" (https://arxiv.org/pdf/2204.12511)</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The float tensor with predictions as unnormalized scores (often referred to as logits) for each example.</p> required <code>target</code> <code>Tensor</code> <p>A float tensor with the same shape as inputs. It stores the binary classification label for each element in inputs (0 for the negative class and 1 for the positive class).</p> required <code>alpha</code> <code>float</code> <p>The weighting factor, which must be in the range <code>[0, 1]</code>. This parameter is ignored if negative.</p> <code>1.0</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed Poly-1 loss value. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import binary_poly1_loss_with_logits\n&gt;&gt;&gt; loss = binary_poly1_loss_with_logits(\n...     torch.randn(2, 4, requires_grad=True),\n...     torch.tensor([[1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0]]),\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.check_loss_reduction_strategy","title":"sonnix.functional.check_loss_reduction_strategy","text":"<pre><code>check_loss_reduction_strategy(reduction: str) -&gt; None\n</code></pre> <p>Check if the provided reduction ia a valid loss reduction.</p> <p>The valid reduction values are <code>'mean'</code>, <code>'none'</code>,  <code>'sum'</code>, and <code>'batchmean'</code>.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>str</code> <p>The reduction strategy to check.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the provided reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from sonnix.functional import check_loss_reduction_strategy\n&gt;&gt;&gt; check_loss_reduction_strategy(\"mean\")\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.classical_relative_indicator","title":"sonnix.functional.classical_relative_indicator","text":"<pre><code>classical_relative_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the classical relative change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import classical_relative_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = classical_relative_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[2., 1., 0.],\n        [3., 5., 1.]])\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.general_robust_regression_loss","title":"sonnix.functional.general_robust_regression_loss","text":"<pre><code>general_robust_regression_loss(\n    prediction: Tensor,\n    target: Tensor,\n    alpha: float = 2.0,\n    scale: float = 1.0,\n    max: float | None = None,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the general robust regression loss a.k.a. Barron robust loss.</p> <p>Based on the paper:</p> <pre><code>A General and Adaptive Robust Loss Function\nJonathan T. Barron\nCVPR 2019 (https://arxiv.org/abs/1701.03077)\n</code></pre> Note <p>The \"adaptative\" part of the loss is not implemented in this     function.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>alpha</code> <code>float</code> <p>The shape parameter that controls the robustness of the loss.</p> <code>2.0</code> <code>scale</code> <code>float</code> <p>The scale parameter that controls the size of the loss's quadratic bowl near 0.</p> <code>1.0</code> <code>max</code> <code>float | None</code> <p>The max value to clip the loss before to compute the reduction. <code>None</code> means no clipping is used.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The loss. The shape of the tensor depends on the reduction strategy.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import general_robust_regression_loss\n&gt;&gt;&gt; loss = general_robust_regression_loss(\n...     torch.randn(2, 4, requires_grad=True), torch.randn(2, 4)\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.geometric_mean_indicator","title":"sonnix.functional.geometric_mean_indicator","text":"<pre><code>geometric_mean_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the geometric mean change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import geometric_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = geometric_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[0.0000, 1.0000, 0.0000],\n        [3.0000, 2.2361, 1.0000]], grad_fn=&lt;SqrtBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.log_cosh_loss","title":"sonnix.functional.log_cosh_loss","text":"<pre><code>log_cosh_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n    scale: float = 1.0,\n) -&gt; Tensor\n</code></pre> <p>Compute the logarithm of the hyperbolic cosine of the prediction error.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>scale</code> <code>float</code> <p>The scale factor.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The logarithm of the hyperbolic cosine of the prediction error.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import log_cosh_loss\n&gt;&gt;&gt; loss = log_cosh_loss(torch.randn(3, 5, requires_grad=True), torch.randn(3, 5))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.maximum_mean_indicator","title":"sonnix.functional.maximum_mean_indicator","text":"<pre><code>maximum_mean_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the maximum mean change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import maximum_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = maximum_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[2., 1., 1.],\n        [3., 5., 1.]], grad_fn=&lt;MaximumBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.minimum_mean_indicator","title":"sonnix.functional.minimum_mean_indicator","text":"<pre><code>minimum_mean_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the minimum mean change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import minimum_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = minimum_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[0., 1., 0.],\n        [3., 1., 1.]], grad_fn=&lt;MinimumBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.moment_mean_indicator","title":"sonnix.functional.moment_mean_indicator","text":"<pre><code>moment_mean_indicator(\n    prediction: Tensor, target: Tensor, k: int = 1\n) -&gt; Tensor\n</code></pre> <p>Return the moment mean change of order k.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>k</code> <code>int</code> <p>The order.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import moment_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = moment_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[1.0000, 1.0000, 0.5000],\n        [3.0000, 3.0000, 1.0000]], grad_fn=&lt;PowBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.msle_loss","title":"sonnix.functional.msle_loss","text":"<pre><code>msle_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the mean squared error (MSE) on the logarithmic transformed predictions and targets.</p> <p>This loss is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this loss penalizes an under-predicted estimate greater than an over-predicted estimate.</p> <p>Note: this loss only works with positive values (0 included).</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean squared logarithmic error. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import msle_loss\n&gt;&gt;&gt; loss = msle_loss(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MseLossBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.poisson_regression_loss","title":"sonnix.functional.poisson_regression_loss","text":"<pre><code>poisson_regression_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n    eps: float = 1e-08,\n) -&gt; Tensor\n</code></pre> <p>Compute the Poisson regression loss.</p> <p>Loss Functions and Metrics in Deep Learning https://arxiv.org/pdf/2307.02694</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The count predictions. The values must be positive.</p> required <code>target</code> <code>Tensor</code> <p>The count target values. The values must be positive.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the count is zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The Poisson regression loss. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import poisson_regression_loss\n&gt;&gt;&gt; loss = poisson_regression_loss(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.quantile_regression_loss","title":"sonnix.functional.quantile_regression_loss","text":"<pre><code>quantile_regression_loss(\n    prediction: Tensor,\n    target: Tensor,\n    q: float = 0.5,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the quantile regression loss.</p> <p>Loss Functions and Metrics in Deep Learning https://arxiv.org/pdf/2307.02694</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>q</code> <code>float</code> <p>The quantile value. <code>q=0.5</code> is equivalent to the Mean Absolute Error (MAE).</p> <code>0.5</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The quantile regression loss. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import quantile_regression_loss\n&gt;&gt;&gt; loss = quantile_regression_loss(\n...     torch.randn(2, 4, requires_grad=True), torch.randn(2, 4)\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.reduce_loss","title":"sonnix.functional.reduce_loss","text":"<pre><code>reduce_loss(tensor: Tensor, reduction: str) -&gt; Tensor\n</code></pre> <p>Return the reduced loss.</p> <p>This function is designed to be used with loss functions.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor to reduce.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  <code>'sum'</code>, and <code>'batchmean'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed, <code>'batchmean'</code>: the sum will be divided by the size of the first dimension.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The reduced tensor. The shape of the tensor depends on the reduction strategy.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import reduce_loss\n&gt;&gt;&gt; tensor = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0]])\n&gt;&gt;&gt; reduce_loss(tensor, \"none\")\ntensor([[0., 1., 2.],\n        [3., 4., 5.]])\n&gt;&gt;&gt; reduce_loss(tensor, \"sum\")\ntensor(15.)\n&gt;&gt;&gt; reduce_loss(tensor, \"mean\")\ntensor(2.5000)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.relative_loss","title":"sonnix.functional.relative_loss","text":"<pre><code>relative_loss(\n    loss: Tensor,\n    indicator: Tensor,\n    reduction: str = \"mean\",\n    eps: float = 1e-08,\n) -&gt; Tensor\n</code></pre> <p>Compute the relative loss.</p> <p>The indicators are designed based on https://en.wikipedia.org/wiki/Relative_change#Indicators_of_relative_change.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Tensor</code> <p>The loss values. The tensor must have the same shape as the target.</p> required <code>indicator</code> <code>Tensor</code> <p>The indicator values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the indicator is zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed relative loss.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the loss and indicator shapes do not match.</p> <code>ValueError</code> <p>if the reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import relative_loss\n&gt;&gt;&gt; prediction = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; target = torch.randn(3, 5)\n&gt;&gt;&gt; loss = relative_loss(\n...     loss=torch.nn.functional.mse_loss(prediction, target, reduction=\"none\"),\n...     indicator=classical_relative_indicator(prediction, target),\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.reversed_relative_indicator","title":"sonnix.functional.reversed_relative_indicator","text":"<pre><code>reversed_relative_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the reversed relative change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional.loss import reversed_relative_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = reversed_relative_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[0., 1., 1.],\n        [3., 1., 1.]], grad_fn=&lt;AbsBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.safe_exp","title":"sonnix.functional.safe_exp","text":"<pre><code>safe_exp(input: Tensor, max: float = 20.0) -&gt; Tensor\n</code></pre> <p>Compute safely the exponential of the elements.</p> <p>The values that are higher than the specified minimum value are set to this maximum value. Using a not too large positive value leads to an output tensor without Inf.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>max</code> <code>float</code> <p>The maximum value.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor with the exponential of the elements.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import safe_exp\n&gt;&gt;&gt; output = safe_exp(torch.tensor([1.0, 10.0, 100.0, 1000.0]))\n&gt;&gt;&gt; output\ntensor([2.7183e+00, 2.2026e+04, 4.8517e+08, 4.8517e+08])\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.safe_log","title":"sonnix.functional.safe_log","text":"<pre><code>safe_log(input: Tensor, min: float = 1e-08) -&gt; Tensor\n</code></pre> <p>Compute safely the logarithm natural logarithm of the elements.</p> <p>The values that are lower than the specified minimum value are set to this minimum value. Using a small positive value leads to an output tensor without NaN or Inf.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>min</code> <code>float</code> <p>The minimum value.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor with the natural logarithm of the elements.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import safe_log\n&gt;&gt;&gt; safe_log(torch.tensor([1e-4, 1e-5, 1e-6, 1e-8, 1e-9, 1e-10]))\ntensor([ -9.2103, -11.5129, -13.8155, -18.4207, -18.4207, -18.4207])\n</code></pre>"},{"location":"refs/functional/#sonnix.functional.symmetric_absolute_relative_error","title":"sonnix.functional.symmetric_absolute_relative_error","text":"<pre><code>symmetric_absolute_relative_error(\n    prediction: Tensor, target: Tensor, eps: float = 1e-08\n) -&gt; Tensor\n</code></pre> <p>Compute the element-wise symmetric absolute relative error between the predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The tensor of predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor, which must have the same shape and data type as <code>prediction</code>.</p> required <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the target is zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The symmetric absolute relative error tensor, which has the same shape and data type as the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.functional import symmetric_absolute_relative_error\n&gt;&gt;&gt; symmetric_absolute_relative_error(torch.eye(2), torch.ones(2, 2))\ntensor([[0., 2.],\n        [2., 0.]])\n</code></pre>"},{"location":"refs/modules/","title":"sonnix.modules","text":""},{"location":"refs/modules/#sonnix.modules","title":"sonnix.modules","text":"<p>Contain modules.</p>"},{"location":"refs/modules/#sonnix.modules.Asinh","title":"sonnix.modules.Asinh","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the inverse hyperbolic sine (arcsinh) of the elements.</p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Asinh\n&gt;&gt;&gt; m = Asinh()\n&gt;&gt;&gt; m\nAsinh()\n&gt;&gt;&gt; out = m(torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 2.0, 4.0]]))\n&gt;&gt;&gt; out\ntensor([[-0.8814,  0.0000,  0.8814],\n        [-1.4436,  1.4436,  2.0947]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.AverageFusion","title":"sonnix.modules.AverageFusion","text":"<p>               Bases: <code>SumFusion</code></p> <p>Implement a layer to average the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import AverageFusion\n&gt;&gt;&gt; module = AverageFusion()\n&gt;&gt;&gt; module\nAverageFusion(normalized=True)\n&gt;&gt;&gt; x1 = torch.tensor([[2.0, 3.0, 4.0], [5.0, 6.0, 7.0]], requires_grad=True)\n&gt;&gt;&gt; x2 = torch.tensor([[12.0, 13.0, 14.0], [15.0, 16.0, 17.0]], requires_grad=True)\n&gt;&gt;&gt; out = module(x1, x2)\n&gt;&gt;&gt; out\ntensor([[ 7.,  8.,  9.],\n        [10., 11., 12.]], grad_fn=&lt;DivBackward0&gt;)\n&gt;&gt;&gt; out.mean().backward()\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.BaseAlphaActivation","title":"sonnix.modules.BaseAlphaActivation","text":"<p>               Bases: <code>Module</code></p> <p>Define a base class to implement an activation layer with a learnable parameter <code>alpha</code>.</p> <p>When called without arguments, the activation layer uses a single parameter <code>alpha</code> across all input channels. If called with a first argument, a separate <code>alpha</code> is used for each input channel.</p> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import MultiQuadratic\n&gt;&gt;&gt; m = MultiQuadratic()\n&gt;&gt;&gt; m\nMultiQuadratic(num_parameters=1, learnable=True)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[1.0000, 0.7071, 0.4472, 0.3162],\n        [0.2425, 0.1961, 0.1644, 0.1414]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.Clamp","title":"sonnix.modules.Clamp","text":"<p>               Bases: <code>Module</code></p> <p>Implement a module to clamp all elements in input into the range <code>[min, max]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>min</code> <code>float | None</code> <p>The lower-bound of the range to be clamped to. <code>None</code> means there is no minimum value.</p> <code>-1.0</code> <code>max</code> <code>float | None</code> <p>The upper-bound of the range to be clamped to. <code>None</code> means there is no maximum value.</p> <code>1.0</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Clamp\n&gt;&gt;&gt; m = Clamp(min=-1, max=2)\n&gt;&gt;&gt; m\nClamp(min=-1, max=2)\n&gt;&gt;&gt; out = m(torch.tensor([[-2.0, -1.0, 0.0], [1.0, 2.0, 3.0]]))\n&gt;&gt;&gt; out\ntensor([[-1., -1.,  0.], [ 1.,  2.,  2.]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.ConcatFusion","title":"sonnix.modules.ConcatFusion","text":"<p>               Bases: <code>Module</code></p> <p>Implement a module to concatenate inputs.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The fusion dimension. <code>-1</code> means the last dimension.</p> <code>-1</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import ConcatFusion\n&gt;&gt;&gt; module = ConcatFusion()\n&gt;&gt;&gt; module\nConcatFusion(dim=-1)\n&gt;&gt;&gt; x1 = torch.tensor([[2.0, 3.0, 4.0], [5.0, 6.0, 7.0]], requires_grad=True)\n&gt;&gt;&gt; x2 = torch.tensor([[12.0, 13.0, 14.0], [15.0, 16.0, 17.0]], requires_grad=True)\n&gt;&gt;&gt; out = module(x1, x2)\n&gt;&gt;&gt; out\ntensor([[ 2.,  3.,  4., 12., 13., 14.],\n        [ 5.,  6.,  7., 15., 16., 17.]], grad_fn=&lt;CatBackward0&gt;)\n&gt;&gt;&gt; out.mean().backward()\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.ExU","title":"sonnix.modules.ExU","text":"<p>               Bases: <code>Module</code></p> <p>Implementation of the exp-centered (ExU) layer.</p> <p>This layer was proposed in the following paper:</p> <pre><code>Neural Additive Models: Interpretable Machine Learning with\nNeural Nets.\nAgarwal R., Melnick L., Frosst N., Zhang X., Lengerich B.,\nCaruana R., Hinton G.\nNeurIPS 2021. (https://arxiv.org/pdf/2004.13912.pdf)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>The size of each input sample.</p> required <code>out_features</code> <code>int</code> <p>The size of each output sample.</p> required <code>bias</code> <code>bool</code> <p>If set to <code>False</code>, the layer will not learn an additive bias.</p> <code>True</code> <code>device</code> <code>device | None</code> <p>The device where to initialize the layer's parameters.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>The data type of the layer's parameters.</p> <code>None</code> Shape <ul> <li>Input: <code>(*, in_features)</code>, where <code>*</code> means any number of     dimensions, including none.</li> <li>Output: <code>(*, out_features)</code>, where <code>*</code> is the same shape     as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import ExU\n&gt;&gt;&gt; m = ExU(4, 6)\n&gt;&gt;&gt; m\nExU(in_features=4, out_features=6, bias=True)\n&gt;&gt;&gt; out = m(torch.rand(6, 4))\n&gt;&gt;&gt; out\ntensor([[...]], grad_fn=&lt;MmBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.ExU.reset_parameters","title":"sonnix.modules.ExU.reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters.</p> <p>As indicated in page 4 of the paper, the weights are initialed using a normal distribution <code>N(4.0; 0.5)</code>. The biases are initialized to <code>0</code></p>"},{"location":"refs/modules/#sonnix.modules.Exp","title":"sonnix.modules.Exp","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the exponential of the input.</p> <p>This module is equivalent to  <code>exp(input)</code></p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Exp\n&gt;&gt;&gt; m = Exp()\n&gt;&gt;&gt; m\nExp()\n&gt;&gt;&gt; out = m(torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 2.0, 3.0]]))\n&gt;&gt;&gt; out\ntensor([[ 0.3679,  1.0000,  2.7183],\n        [ 0.1353,  7.3891, 20.0855]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.ExpSin","title":"sonnix.modules.ExpSin","text":"<p>               Bases: <code>BaseAlphaActivation</code></p> <p>Implement the ExpSin activation layer.</p> <p>Formula: <code>exp(-sin(alpha * x))</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import ExpSin\n&gt;&gt;&gt; m = ExpSin()\n&gt;&gt;&gt; m\nExpSin(num_parameters=1, learnable=True)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[1.0000, 2.3198, 2.4826, 1.1516],\n        [0.4692, 0.3833, 0.7562, 1.9290]], grad_fn=&lt;ExpBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.Expm1","title":"sonnix.modules.Expm1","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the exponential of the elements minus 1 of input.</p> <p>This module is equivalent to  <code>exp(input) - 1</code></p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Expm1\n&gt;&gt;&gt; m = Expm1()\n&gt;&gt;&gt; m\nExpm1()\n&gt;&gt;&gt; out = m(torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 2.0, 4.0]]))\n&gt;&gt;&gt; out\ntensor([[-0.6321,  0.0000,  1.7183],\n        [-0.8647,  6.3891, 53.5981]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.Gaussian","title":"sonnix.modules.Gaussian","text":"<p>               Bases: <code>BaseAlphaActivation</code></p> <p>Implement the Gaussian activation layer.</p> <p>Formula: <code>exp(-0.5 * x^2 / alpha^2)</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Gaussian\n&gt;&gt;&gt; m = Gaussian()\n&gt;&gt;&gt; m\nGaussian(num_parameters=1, learnable=True)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[1.0000e+00, 6.0653e-01, 1.3534e-01, 1.1109e-02],\n        [3.3546e-04, 3.7267e-06, 1.5230e-08, 2.2897e-11]], grad_fn=&lt;ExpBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.Laplacian","title":"sonnix.modules.Laplacian","text":"<p>               Bases: <code>BaseAlphaActivation</code></p> <p>Implement the Laplacian activation layer.</p> <p>Formula: <code>exp(-|x| / alpha)</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Laplacian\n&gt;&gt;&gt; m = Laplacian()\n&gt;&gt;&gt; m\nLaplacian(num_parameters=1, learnable=True)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[1.0000e+00, 3.6788e-01, 1.3534e-01, 4.9787e-02],\n        [1.8316e-02, 6.7379e-03, 2.4788e-03, 9.1188e-04]], grad_fn=&lt;ExpBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.Log","title":"sonnix.modules.Log","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the natural logarithm of the input.</p> <p>This module is equivalent to  <code>log(input)</code></p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Log\n&gt;&gt;&gt; m = Log()\n&gt;&gt;&gt; m\nLog()\n&gt;&gt;&gt; out = m(torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]))\n&gt;&gt;&gt; out\ntensor([[0.0000, 0.6931, 1.0986],\n        [1.3863, 1.6094, 1.7918]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.Log1p","title":"sonnix.modules.Log1p","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the natural logarithm of <code>(1 + input)</code>.</p> <p>This module is equivalent to  <code>log(1 + input)</code></p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Log1p\n&gt;&gt;&gt; m = Log1p()\n&gt;&gt;&gt; m\nLog1p()\n&gt;&gt;&gt; out = m(torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0]]))\n&gt;&gt;&gt; out\ntensor([[0.0000, 0.6931, 1.0986],\n        [1.3863, 1.6094, 1.7918]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.MultiQuadratic","title":"sonnix.modules.MultiQuadratic","text":"<p>               Bases: <code>BaseAlphaActivation</code></p> <p>Implement the Multi Quadratic activation layer.</p> <p>Formula: <code>1 / sqrt(1 + (alpha * x)^2)</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import MultiQuadratic\n&gt;&gt;&gt; m = MultiQuadratic()\n&gt;&gt;&gt; m\nMultiQuadratic(num_parameters=1, learnable=True)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[1.0000, 0.7071, 0.4472, 0.3162],\n        [0.2425, 0.1961, 0.1644, 0.1414]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.MulticlassFlatten","title":"sonnix.modules.MulticlassFlatten","text":"<p>               Bases: <code>Module</code></p> <p>Implement a wrapper to flat the multiclass inputs of a <code>torch.nn.Module</code>.</p> <p>The input prediction tensor shape is <code>(d1, d2, ..., dn, C)</code> and is reshaped to <code>(d1 * d2 * ... * dn, C)</code>. The input target tensor shape is <code>(d1, d2, ..., dn)</code> and is reshaped to <code>(d1 * d2 * ... * dn,)</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import MulticlassFlatten\n&gt;&gt;&gt; m = MulticlassFlatten(torch.nn.CrossEntropyLoss())\n&gt;&gt;&gt; m\nMulticlassFlatten(\n  (module): CrossEntropyLoss()\n)\n&gt;&gt;&gt; out = m(torch.ones(6, 2, 4, requires_grad=True), torch.zeros(6, 2, dtype=torch.long))\n&gt;&gt;&gt; out\ntensor(1.3863, grad_fn=&lt;NllLossBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.MultiplicationFusion","title":"sonnix.modules.MultiplicationFusion","text":"<p>               Bases: <code>Module</code></p> <p>Implement a fusion layer that multiplies the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import MultiplicationFusion\n&gt;&gt;&gt; module = MultiplicationFusion()\n&gt;&gt;&gt; module\nMultiplicationFusion()\n&gt;&gt;&gt; x1 = torch.tensor([[2.0, 3.0, 4.0], [5.0, 6.0, 7.0]], requires_grad=True)\n&gt;&gt;&gt; x2 = torch.tensor([[12.0, 13.0, 14.0], [15.0, 16.0, 17.0]], requires_grad=True)\n&gt;&gt;&gt; out = module(x1, x2)\n&gt;&gt;&gt; out\ntensor([[ 24.,  39.,  56.],\n        [ 75.,  96., 119.]], grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; out.mean().backward()\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.NLinear","title":"sonnix.modules.NLinear","text":"<p>               Bases: <code>Module</code></p> <p>Implement N separate linear layers.</p> <p>Technically, <code>NLinear(n, in, out)</code> is just a layout of <code>n</code> linear layers <code>torch.nn.Linear(in, out)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of separate linear layers.</p> required <code>in_features</code> <code>int</code> <p>The size of each input sample.</p> required <code>out_features</code> <code>int</code> <p>The size of each output sample.</p> required <code>bias</code> <code>bool</code> <p>If set to <code>False</code>, the layer will not learn an additive bias.</p> <code>True</code> Shape <ul> <li>Input: <code>(*, n, in_features)</code>, where <code>*</code> means any number of     dimensions.</li> <li>Output: <code>(*, n, out_features)</code>,  where <code>*</code> has     the same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import NLinear\n&gt;&gt;&gt; # Example with 1 feature\n&gt;&gt;&gt; m = NLinear(n=3, in_features=4, out_features=6)\n&gt;&gt;&gt; m\nNLinear(n=3, in_features=4, out_features=6, bias=True)\n&gt;&gt;&gt; out = m(torch.randn(2, 3, 4))\n&gt;&gt;&gt; out.shape\ntorch.Size([2, 3, 6])\n&gt;&gt;&gt; out = m(torch.randn(2, 5, 3, 4))\n&gt;&gt;&gt; out.shape\ntorch.Size([2, 5, 3, 6])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.Quadratic","title":"sonnix.modules.Quadratic","text":"<p>               Bases: <code>BaseAlphaActivation</code></p> <p>Implement the Quadratic activation layer.</p> <p>Formula: <code>1 / (1 + (alpha * x)^2)</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Quadratic\n&gt;&gt;&gt; m = Quadratic()\n&gt;&gt;&gt; m\nQuadratic(num_parameters=1, learnable=True)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[1.0000, 0.5000, 0.2000, 0.1000],\n        [0.0588, 0.0385, 0.0270, 0.0200]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.ReLUn","title":"sonnix.modules.ReLUn","text":"<p>               Bases: <code>Module</code></p> <p>Implement the ReLU-n module.</p> <p>The ReLU-n equation is: <code>ReLUn(x, n)=min(max(0,x),n)</code></p> <p>Parameters:</p> Name Type Description Default <code>max</code> <code>float</code> <p>The maximum value a.k.a. <code>n</code> in the equation above.</p> <code>1.0</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import ReLUn\n&gt;&gt;&gt; m = ReLUn(max=5)\n&gt;&gt;&gt; m\nReLUn(max=5.0)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[0., 1., 2., 3.],\n        [4., 5., 5., 5.]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.ResidualBlock","title":"sonnix.modules.ResidualBlock","text":"<p>               Bases: <code>Module</code></p> <p>Implementation of a residual block.</p> <p>Parameters:</p> Name Type Description Default <code>residual</code> <code>Module | dict</code> <p>The residual mapping module or its configuration (dictionary).</p> required <code>skip</code> <code>Module | dict | None</code> <p>The skip mapping module or its configuration (dictionary). If <code>None</code>, the <code>Identity</code> module is used.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from sonnix.modules import ResidualBlock\n&gt;&gt;&gt; m = ResidualBlock(residual=nn.Sequential(nn.Linear(4, 6), nn.ReLU(), nn.Linear(6, 4)))\n&gt;&gt;&gt; m\nResidualBlock(\n  (residual): Sequential(\n    (0): Linear(in_features=4, out_features=6, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=6, out_features=4, bias=True)\n  )\n  (skip): Identity()\n)\n&gt;&gt;&gt; out = m(torch.rand(6, 4))\n&gt;&gt;&gt; out\ntensor([[...]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.SafeExp","title":"sonnix.modules.SafeExp","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the exponential of the elements.</p> <p>The values that are higher than the specified minimum value are set to this maximum value. Using a not too large positive value leads to an output tensor without Inf.</p> <p>Parameters:</p> Name Type Description Default <code>max</code> <code>float</code> <p>The maximum value before to compute the exponential.</p> <code>20.0</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import SafeExp\n&gt;&gt;&gt; m = SafeExp()\n&gt;&gt;&gt; m\nSafeExp(max=20.0)\n&gt;&gt;&gt; out = m(torch.tensor([[0.01, 0.1, 1.0], [10.0, 100.0, 1000.0]]))\n&gt;&gt;&gt; out\ntensor([[1.0101e+00, 1.1052e+00, 2.7183e+00],\n        [2.2026e+04, 4.8517e+08, 4.8517e+08]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.SafeLog","title":"sonnix.modules.SafeLog","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the logarithm natural of the elements.</p> <p>The values that are lower than the specified minimum value are set to this minimum value. Using a small positive value leads to an output tensor without NaN or Inf.</p> <p>Parameters:</p> Name Type Description Default <code>min</code> <code>float</code> <p>The minimum value before to compute the logarithm natural.</p> <code>1e-08</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import SafeLog\n&gt;&gt;&gt; m = SafeLog()\n&gt;&gt;&gt; m\nSafeLog(min=1e-08)\n&gt;&gt;&gt; out = m(torch.tensor([[1e-4, 1e-5, 1e-6], [1e-8, 1e-9, 1e-10]]))\n&gt;&gt;&gt; out\ntensor([[ -9.2103, -11.5129, -13.8155],\n        [-18.4207, -18.4207, -18.4207]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.Sin","title":"sonnix.modules.Sin","text":"<p>               Bases: <code>Module</code></p> <p>Implement the sine activation layer.</p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Sin\n&gt;&gt;&gt; m = Sin()\n&gt;&gt;&gt; m\nSin()\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[ 0.0000,  0.8415,  0.9093,  0.1411],\n        [-0.7568, -0.9589, -0.2794,  0.6570]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.Sinh","title":"sonnix.modules.Sinh","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the hyperbolic sine (sinh) of the elements.</p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Sinh\n&gt;&gt;&gt; m = Sinh()\n&gt;&gt;&gt; m\nSinh()\n&gt;&gt;&gt; out = m(torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 2.0, 4.0]]))\n&gt;&gt;&gt; out\ntensor([[-1.1752,  0.0000,  1.1752],\n        [-3.6269,  3.6269, 27.2899]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.Snake","title":"sonnix.modules.Snake","text":"<p>               Bases: <code>Module</code></p> <p>Implement the Snake activation layer.</p> <p>Snake was proposed in the following paper:</p> <pre><code>Neural Networks Fail to Learn Periodic Functions and How to Fix It.\nZiyin L., Hartwig T., Ueda M.\nNeurIPS, 2020. (http://arxiv.org/pdf/2006.08195)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>frequency</code> <code>float</code> <p>The frequency.</p> <code>1.0</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Snake\n&gt;&gt;&gt; m = Snake()\n&gt;&gt;&gt; m\nSnake(frequency=1.0)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[0.0000, 1.7081, 2.8268, 3.0199],\n        [4.5728, 5.9195, 6.0781, 7.4316]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.SquaredReLU","title":"sonnix.modules.SquaredReLU","text":"<p>               Bases: <code>Module</code></p> <p>Implement the Squared ReLU.</p> <p>Squared ReLU is defined in the following paper:</p> <pre><code>Primer: Searching for Efficient Transformers for Language Modeling.\nSo DR., Ma\u0144ke W., Liu H., Dai Z., Shazeer N., Le QV.\nNeurIPS, 2021. (https://arxiv.org/pdf/2109.08668.pdf)\n</code></pre> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import SquaredReLU\n&gt;&gt;&gt; m = SquaredReLU()\n&gt;&gt;&gt; m\nSquaredReLU()\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[ 0.,  1.,  4.,  9.],\n        [16., 25., 36., 49.]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.Squeeze","title":"sonnix.modules.Squeeze","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to squeeze the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int | None</code> <p>The dimension to squeeze the input tensor. If <code>None</code>, all the dimensions of the input tensor of size 1 are removed.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import Squeeze\n&gt;&gt;&gt; m = Squeeze()\n&gt;&gt;&gt; m\nSqueeze(dim=None)\n&gt;&gt;&gt; out = m(torch.ones(2, 1, 3, 1))\n&gt;&gt;&gt; out.shape\ntorch.Size([2, 3])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.SumFusion","title":"sonnix.modules.SumFusion","text":"<p>               Bases: <code>Module</code></p> <p>Implement a layer to sum the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>normalized</code> <code>bool</code> <p>The output is normalized by the number of inputs.</p> <code>False</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import SumFusion\n&gt;&gt;&gt; module = SumFusion()\n&gt;&gt;&gt; module\nSumFusion(normalized=False)\n&gt;&gt;&gt; x1 = torch.tensor([[2.0, 3.0, 4.0], [5.0, 6.0, 7.0]], requires_grad=True)\n&gt;&gt;&gt; x2 = torch.tensor([[12.0, 13.0, 14.0], [15.0, 16.0, 17.0]], requires_grad=True)\n&gt;&gt;&gt; out = module(x1, x2)\n&gt;&gt;&gt; out\ntensor([[14., 16., 18.],\n        [20., 22., 24.]], grad_fn=&lt;AddBackward0&gt;)\n&gt;&gt;&gt; out.mean().backward()\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.ToFloat","title":"sonnix.modules.ToFloat","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to convert a tensor to a float tensor.</p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import ToFloat\n&gt;&gt;&gt; m = ToFloat()\n&gt;&gt;&gt; m\nToFloat()\n&gt;&gt;&gt; out = m(torch.tensor([[2, -1, 0], [1, 2, 3]]))\n&gt;&gt;&gt; out\ntensor([[ 2., -1.,  0.],\n        [ 1.,  2.,  3.]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.ToLong","title":"sonnix.modules.ToLong","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to convert a tensor to a long tensor.</p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import ToLong\n&gt;&gt;&gt; m = ToLong()\n&gt;&gt;&gt; m\nToLong()\n&gt;&gt;&gt; out = m(torch.tensor([[2.0, -1.0, 0.0], [1.0, 2.0, 3.0]]))\n&gt;&gt;&gt; out\ntensor([[ 2, -1,  0],\n        [ 1,  2,  3]])\n</code></pre>"},{"location":"refs/modules/#sonnix.modules.View","title":"sonnix.modules.View","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to return a new tensor with the same data as the input tensor but of a different shape.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...] | list[int]</code> <p>The desired shape.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.modules import View\n&gt;&gt;&gt; m = View(shape=(-1, 2, 3))\n&gt;&gt;&gt; m\nView(shape=(-1, 2, 3))\n&gt;&gt;&gt; out = m(torch.ones(4, 5, 2, 3))\n&gt;&gt;&gt; out.shape\ntorch.Size([20, 2, 3])\n</code></pre>"},{"location":"refs/root/","title":"sonnix","text":""},{"location":"refs/root/#sonnix","title":"sonnix","text":"<p>Root package.</p>"},{"location":"refs/testing/","title":"sonnix.testing","text":""},{"location":"refs/testing/#sonnix.testing","title":"sonnix.testing","text":"<p>Define some utility functions for testing.</p>"},{"location":"refs/utils/","title":"sonnix.utils","text":""},{"location":"refs/utils/#sonnix.utils.device","title":"sonnix.utils.device","text":"<p>Contain utility functions to manage the device(s) of a <code>torch.nn.Module</code>.</p>"},{"location":"refs/utils/#sonnix.utils.device.get_module_device","title":"sonnix.utils.device.get_module_device","text":"<pre><code>get_module_device(module: Module) -&gt; device\n</code></pre> <p>Get the device used by this module.</p> <p>This function assumes the module uses a single device. If the module uses several devices, you should use <code>get_module_devices</code>. It returns <code>torch.device('cpu')</code> if the model does not have parameters.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <p>Returns:</p> Type Description <code>device</code> <p>The device</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.device import get_module_device\n&gt;&gt;&gt; get_module_device(torch.nn.Linear(4, 6))\ndevice(type='cpu')\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.device.get_module_devices","title":"sonnix.utils.device.get_module_devices","text":"<pre><code>get_module_devices(module: Module) -&gt; list[device]\n</code></pre> <p>Get the devices used in a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <p>Returns:</p> Type Description <code>list[device]</code> <p>The list of <code>torch.device</code>s used in the module.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.device import get_module_devices\n&gt;&gt;&gt; get_module_devices(torch.nn.Linear(4, 6))\n[device(type='cpu')]\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.device.is_module_on_device","title":"sonnix.utils.device.is_module_on_device","text":"<pre><code>is_module_on_device(module: Module, device: device) -&gt; bool\n</code></pre> <p>Indicate if all the parameters of a module are on the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <code>device</code> <code>device</code> <p>The device.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if all the parameters of the module are on the specified device, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.device import is_module_on_device\n&gt;&gt;&gt; is_module_on_device(torch.nn.Linear(4, 6), torch.device(\"cpu\"))\nTrue\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.imports","title":"sonnix.utils.imports","text":"<p>Implement some utility functions to manage optional dependencies.</p>"},{"location":"refs/utils/#sonnix.utils.imports.check_objectory","title":"sonnix.utils.imports.check_objectory","text":"<pre><code>check_objectory() -&gt; None\n</code></pre> <p>Check if the <code>objectory</code> package is installed.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the <code>objectory</code> package is not installed.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from sonnix.utils.imports import check_objectory\n&gt;&gt;&gt; check_objectory()\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.imports.is_objectory_available","title":"sonnix.utils.imports.is_objectory_available","text":"<pre><code>is_objectory_available() -&gt; bool\n</code></pre> <p>Indicate if the <code>objectory</code> package is installed or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if <code>objectory</code> is available otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from sonnix.utils.imports import is_objectory_available\n&gt;&gt;&gt; is_objectory_available()\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.imports.objectory_available","title":"sonnix.utils.imports.objectory_available","text":"<pre><code>objectory_available(\n    fn: Callable[..., Any],\n) -&gt; Callable[..., Any]\n</code></pre> <p>Implement a decorator to execute a function only if <code>objectory</code> package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., Any]</code> <p>Specifies the function to execute.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>A wrapper around <code>fn</code> if <code>objectory</code> package is installed, otherwise <code>None</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from sonnix.utils.imports import objectory_available\n&gt;&gt;&gt; @objectory_available\n... def my_function(n: int = 0) -&gt; int:\n...     return 42 + n\n...\n&gt;&gt;&gt; my_function()\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.imports.raise_error_objectory_missing","title":"sonnix.utils.imports.raise_error_objectory_missing","text":"<pre><code>raise_error_objectory_missing() -&gt; NoReturn\n</code></pre> <p>Raise a RuntimeError to indicate the <code>objectory</code> package is missing.</p>"},{"location":"refs/utils/#sonnix.utils.iterator","title":"sonnix.utils.iterator","text":"<p>Contain iterators on <code>torch.nn.Module</code>.</p>"},{"location":"refs/utils/#sonnix.utils.iterator.get_named_modules","title":"sonnix.utils.iterator.get_named_modules","text":"<pre><code>get_named_modules(\n    module: Module, depth: int = 0\n) -&gt; Generator[tuple[str, Module]]\n</code></pre> <p>Return an iterator over the modules, yielding both the name of the module as well as the module itself.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The input module.</p> required <code>depth</code> <code>int</code> <p>The maximum depth of module to yield.</p> <code>0</code> <p>Returns:</p> Type Description <code>Generator[tuple[str, Module]]</code> <p>The iterator over the modules and their names.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.iterator import get_named_modules\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; named_modules = list(get_named_modules(module))\n&gt;&gt;&gt; named_modules\n[('[root]', Linear(in_features=4, out_features=6, bias=True))]\n&gt;&gt;&gt; module = torch.nn.Sequential(\n...     torch.nn.Linear(4, 6), torch.nn.ReLU(), torch.nn.Linear(6, 3)\n... )\n&gt;&gt;&gt; named_modules = list(get_named_modules(module))\n&gt;&gt;&gt; named_modules\n[('[root]', Sequential(\n  (0): Linear(in_features=4, out_features=6, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=6, out_features=3, bias=True)\n))]\n&gt;&gt;&gt; named_modules = list(get_named_modules(module, depth=1))\n&gt;&gt;&gt; named_modules\n[('[root]', Sequential(\n  (0): Linear(in_features=4, out_features=6, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=6, out_features=3, bias=True)\n)),\n('0', Linear(in_features=4, out_features=6, bias=True)),\n('1', ReLU()), ('2', Linear(in_features=6, out_features=3, bias=True))]\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.loss","title":"sonnix.utils.loss","text":"<p>Contain utility functions to check if the loss is decreasing.</p>"},{"location":"refs/utils/#sonnix.utils.loss.is_loss_decreasing","title":"sonnix.utils.loss.is_loss_decreasing","text":"<pre><code>is_loss_decreasing(\n    module: Module,\n    criterion: Module | Callable[[Tensor, Tensor], Tensor],\n    optimizer: Optimizer,\n    feature: Tensor,\n    target: Tensor,\n    num_iterations: int = 1,\n    random_seed: int = 10772155803920552556,\n) -&gt; bool\n</code></pre> <p>Check if the loss decreased after some iterations.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test. The module must have a single input tensor and a single output tensor.</p> required <code>criterion</code> <code>Module | Callable[[Tensor, Tensor], Tensor]</code> <p>The criterion to test.</p> required <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to update the weights of the model.</p> required <code>feature</code> <code>Tensor</code> <p>The input of the module.</p> required <code>target</code> <code>Tensor</code> <p>The target used to compute the loss.</p> required <code>num_iterations</code> <code>int</code> <p>The number of optimization steps.</p> <code>1</code> <code>random_seed</code> <code>int</code> <p>The random seed to make the function deterministic if the module contains randomness.</p> <code>10772155803920552556</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the loss decreased after some iterations, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from sonnix.utils.loss import is_loss_decreasing\n&gt;&gt;&gt; module = nn.Linear(4, 2)\n&gt;&gt;&gt; is_loss_decreasing(\n...     module=module,\n...     criterion=nn.MSELoss(),\n...     optimizer=SGD(module.parameters(), lr=0.01),\n...     feature=torch.rand(4, 4),\n...     target=torch.rand(4, 2),\n... )\nTrue\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.loss.is_loss_decreasing_with_adam","title":"sonnix.utils.loss.is_loss_decreasing_with_adam","text":"<pre><code>is_loss_decreasing_with_adam(\n    module: Module,\n    criterion: Module | Callable[[Tensor, Tensor], Tensor],\n    feature: Tensor,\n    target: Tensor,\n    lr: float = 0.0003,\n    num_iterations: int = 1,\n    random_seed: int = 10772155803920552556,\n) -&gt; bool\n</code></pre> <p>Check if the loss decreased after some iterations.</p> <p>The module is trained with the Adam optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test. The module must have a single input tensor and a single output tensor.</p> required <code>criterion</code> <code>Module | Callable[[Tensor, Tensor], Tensor]</code> <p>The criterion to test.</p> required <code>feature</code> <code>Tensor</code> <p>The input of the module.</p> required <code>target</code> <code>Tensor</code> <p>The target used to compute the loss.</p> required <code>lr</code> <code>float</code> <p>The learning rate.</p> <code>0.0003</code> <code>num_iterations</code> <code>int</code> <p>The number of optimization steps.</p> <code>1</code> <code>random_seed</code> <code>int</code> <p>The random seed to make the function deterministic if the module contains randomness.</p> <code>10772155803920552556</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the loss decreased after some iterations, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from sonnix.utils.loss import is_loss_decreasing_with_adam\n&gt;&gt;&gt; is_loss_decreasing_with_adam(\n...     module=nn.Linear(4, 2),\n...     criterion=nn.MSELoss(),\n...     feature=torch.rand(4, 4),\n...     target=torch.rand(4, 2),\n...     lr=0.0003,\n... )\nTrue\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.loss.is_loss_decreasing_with_sgd","title":"sonnix.utils.loss.is_loss_decreasing_with_sgd","text":"<pre><code>is_loss_decreasing_with_sgd(\n    module: Module,\n    criterion: Module | Callable[[Tensor, Tensor], Tensor],\n    feature: Tensor,\n    target: Tensor,\n    lr: float = 0.01,\n    num_iterations: int = 1,\n    random_seed: int = 10772155803920552556,\n) -&gt; bool\n</code></pre> <p>Check if the loss decreased after some iterations.</p> <p>The module is trained with the <code>torch.optim.SGD</code> optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test. The module must have a single input tensor and a single output tensor.</p> required <code>criterion</code> <code>Module | Callable[[Tensor, Tensor], Tensor]</code> <p>The criterion to test.</p> required <code>feature</code> <code>Tensor</code> <p>The input of the module.</p> required <code>target</code> <code>Tensor</code> <p>The target used to compute the loss.</p> required <code>num_iterations</code> <code>int</code> <p>The number of optimization steps.</p> <code>1</code> <code>lr</code> <code>float</code> <p>The learning rate.</p> <code>0.01</code> <code>random_seed</code> <code>int</code> <p>The random seed to make the function deterministic if the module contains randomness.</p> <code>10772155803920552556</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the loss decreased after some iterations, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from sonnix.utils.loss import is_loss_decreasing_with_adam\n&gt;&gt;&gt; is_loss_decreasing_with_adam(\n...     module=nn.Linear(4, 2),\n...     criterion=nn.MSELoss(),\n...     feature=torch.rand(4, 4),\n...     target=torch.rand(4, 2),\n...     lr=0.01,\n... )\nTrue\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.mode","title":"sonnix.utils.mode","text":"<p>Contain utility functions to manage the mode of a <code>torch.nn.Module</code>.</p>"},{"location":"refs/utils/#sonnix.utils.mode.module_mode","title":"sonnix.utils.mode.module_mode","text":"<pre><code>module_mode(module: Module) -&gt; Generator[None]\n</code></pre> <p>Implement a context manager that restores the mode (train or eval) of every submodule individually.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to restore the mode.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.mode import module_mode\n&gt;&gt;&gt; module = torch.nn.ModuleDict(\n...     {\"module1\": torch.nn.Linear(4, 6), \"module2\": torch.nn.Linear(2, 4).eval()}\n... )\n&gt;&gt;&gt; print(module[\"module1\"].training, module[\"module2\"].training)\nTrue False\n&gt;&gt;&gt; with module_mode(module):\n...     module.eval()\n...     print(module[\"module1\"].training, module[\"module2\"].training)\n...\nModuleDict(\n  (module1): Linear(in_features=4, out_features=6, bias=True)\n  (module2): Linear(in_features=2, out_features=4, bias=True)\n)\nFalse False\n&gt;&gt;&gt; print(module[\"module1\"].training, module[\"module2\"].training)\nTrue False\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.mode.top_module_mode","title":"sonnix.utils.mode.top_module_mode","text":"<pre><code>top_module_mode(module: Module) -&gt; Generator[None]\n</code></pre> <p>Implement a context manager that restores the mode (train or eval) of a given module.</p> <p>This context manager only restores the mode at the top-level.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to restore the mode.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.mode import top_module_mode\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; print(module.training)\nTrue\n&gt;&gt;&gt; with top_module_mode(module):\n...     module.eval()\n...     print(module.training)\n...\nLinear(in_features=4, out_features=6, bias=True)\nFalse\n&gt;&gt;&gt; print(module.training)\nTrue\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.params","title":"sonnix.utils.params","text":"<p>Contain utility functions to analyze and manage <code>torch.nn.Module</code> parameters.</p>"},{"location":"refs/utils/#sonnix.utils.params.freeze_module","title":"sonnix.utils.params.freeze_module","text":"<pre><code>freeze_module(module: Module) -&gt; None\n</code></pre> <p>Freeze the parameters of the given module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to freeze.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.params import freeze_module\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; freeze_module(module)\n&gt;&gt;&gt; for name, param in module.named_parameters():\n...     print(name, param.requires_grad)\n...\nweight False\nbias False\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.params.has_learnable_parameters","title":"sonnix.utils.params.has_learnable_parameters","text":"<pre><code>has_learnable_parameters(module: Module) -&gt; bool\n</code></pre> <p>Indicate if the module has learnable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the module has at least one learnable parameter, <code>False</code> otherwise.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.params import has_learnable_parameters, freeze_module\n&gt;&gt;&gt; has_learnable_parameters(torch.nn.Linear(4, 6))\nTrue\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; freeze_module(module)\n&gt;&gt;&gt; has_learnable_parameters(module)\nFalse\n&gt;&gt;&gt; has_learnable_parameters(torch.nn.Identity())\nFalse\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.params.has_parameters","title":"sonnix.utils.params.has_parameters","text":"<pre><code>has_parameters(module: Module) -&gt; bool\n</code></pre> <p>Indicate if the module has parameters.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the module has at least one parameter, <code>False</code> otherwise.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.params import has_parameters\n&gt;&gt;&gt; has_parameters(torch.nn.Linear(4, 6))\nTrue\n&gt;&gt;&gt; has_parameters(torch.nn.Identity())\nFalse\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.params.num_learnable_parameters","title":"sonnix.utils.params.num_learnable_parameters","text":"<pre><code>num_learnable_parameters(module: Module) -&gt; int\n</code></pre> <p>Return the number of learnable parameters in the module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to compute the number of learnable parameters.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of learnable parameters.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.params import num_learnable_parameters\n&gt;&gt;&gt; num_learnable_parameters(torch.nn.Linear(4, 6))\n30\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; freeze_module(module)\n&gt;&gt;&gt; num_learnable_parameters(module)\n0\n&gt;&gt;&gt; num_learnable_parameters(torch.nn.Identity())\n0\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.params.num_parameters","title":"sonnix.utils.params.num_parameters","text":"<pre><code>num_parameters(module: Module) -&gt; int\n</code></pre> <p>Return the number of parameters in the module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to compute the number of parameters.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of parameters.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.params import num_parameters\n&gt;&gt;&gt; num_parameters(torch.nn.Linear(4, 6))\n30\n&gt;&gt;&gt; num_parameters(torch.nn.Identity())\n0\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.params.unfreeze_module","title":"sonnix.utils.params.unfreeze_module","text":"<pre><code>unfreeze_module(module: Module) -&gt; None\n</code></pre> <p>Unfreeze the parameters of the given module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to unfreeze.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.params import unfreeze_module\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; unfreeze_module(module)\n&gt;&gt;&gt; for name, param in module.named_parameters():\n...     print(name, param.requires_grad)\n...\nweight True\nbias True\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.random","title":"sonnix.utils.random","text":"<p>Contain utility functions to manage randomness.</p>"},{"location":"refs/utils/#sonnix.utils.random.get_random_seed","title":"sonnix.utils.random.get_random_seed","text":"<pre><code>get_random_seed(seed: int) -&gt; int\n</code></pre> <p>Get a random seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>A random seed to make the process reproducible.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>A random seed. The value is between <code>-2 ** 63</code> and <code>2 ** 63 - 1</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from sonnix.utils.random import get_random_seed\n&gt;&gt;&gt; get_random_seed(44)\n6176747449835261347\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.random.get_torch_generator","title":"sonnix.utils.random.get_torch_generator","text":"<pre><code>get_torch_generator(\n    random_seed: int = 1,\n    device: device | str | None = \"cpu\",\n) -&gt; Generator\n</code></pre> <p>Create a <code>torch.Generator</code> initialized with a given seed.</p> <p>Parameters:</p> Name Type Description Default <code>random_seed</code> <code>int</code> <p>A random seed.</p> <code>1</code> <code>device</code> <code>device | str | None</code> <p>The desired device for the generator.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Generator</code> <p>A <code>torch.Generator</code> object.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.random import get_torch_generator\n&gt;&gt;&gt; generator = get_torch_generator(42)\n&gt;&gt;&gt; torch.rand(2, 4, generator=generator)\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936]])\n&gt;&gt;&gt; generator = get_torch_generator(42)\n&gt;&gt;&gt; torch.rand(2, 4, generator=generator)\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936]])\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.random.setup_torch_generator","title":"sonnix.utils.random.setup_torch_generator","text":"<pre><code>setup_torch_generator(\n    generator_or_seed: int | Generator,\n) -&gt; Generator\n</code></pre> <p>Set up a <code>torch.Generator</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>generator_or_seed</code> <code>int | Generator</code> <p>A <code>torch.Generator</code> object or a random seed.</p> required <p>Returns:</p> Type Description <code>Generator</code> <p>A <code>torch.Generator</code> object.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from sonnix.utils.random import setup_torch_generator\n&gt;&gt;&gt; generator = setup_torch_generator(42)\n&gt;&gt;&gt; generator\n&lt;torch._C.Generator object at 0x...&gt;\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.state_dict","title":"sonnix.utils.state_dict","text":"<p>Contain utility functions to manipulate <code>torch.nn.Module</code>'s state dict.</p>"},{"location":"refs/utils/#sonnix.utils.state_dict.find_module_state_dict","title":"sonnix.utils.state_dict.find_module_state_dict","text":"<pre><code>find_module_state_dict(\n    state_dict: dict | list | tuple | set, module_keys: set\n) -&gt; dict\n</code></pre> <p>Try to find automatically the part of the state dict related to a module.</p> <p>The user should specify the set of module's keys: <code>set(module.state_dict().keys())</code>. This function assumes that the set of keys only exists at one location in the state dict. If the set of keys exists at several locations in the state dict, only the first one is returned.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict | list | tuple | set</code> <p>The state dict. This function is called recursively on this input to find the queried state dict.</p> required <code>module_keys</code> <code>set</code> <p>The set of module keys.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The part of the state dict related to a module if it is found, otherwise an empty dict.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.state_dict import find_module_state_dict\n&gt;&gt;&gt; state = {\n...     \"model\": {\n...         \"weight\": 42,\n...         \"network\": {\n...             \"weight\": torch.ones(5, 4),\n...             \"bias\": 2 * torch.ones(5),\n...         },\n...     }\n... }\n&gt;&gt;&gt; module = torch.nn.Linear(4, 5)\n&gt;&gt;&gt; state_dict = find_module_state_dict(state, module_keys=set(module.state_dict().keys()))\n&gt;&gt;&gt; state_dict\n{'weight': tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]]), 'bias': tensor([2., 2., 2., 2., 2.])}\n</code></pre>"},{"location":"refs/utils/#sonnix.utils.state_dict.load_state_dict_to_module","title":"sonnix.utils.state_dict.load_state_dict_to_module","text":"<pre><code>load_state_dict_to_module(\n    state_dict: dict, module: Module, strict: bool = True\n) -&gt; None\n</code></pre> <p>Load a state dict into a given module.</p> <p>This function will automatically try to find the module state dict in the given state dict.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict</code> <p>The state dict.</p> required <code>module</code> <code>Module</code> <p>The module. This function changes the weights of this module.</p> required <code>strict</code> <code>bool</code> <p>whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module's <code>torch.nn.Module.state_dict</code> function.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from sonnix.utils.state_dict import load_state_dict_to_module\n&gt;&gt;&gt; state = {\n...     \"model\": {\n...         \"weight\": 42,\n...         \"network\": {\n...             \"weight\": torch.ones(5, 4),\n...             \"bias\": 2 * torch.ones(5),\n...         },\n...     }\n... }\n&gt;&gt;&gt; module = torch.nn.Linear(4, 5)\n&gt;&gt;&gt; load_state_dict_to_module(state, module)\n&gt;&gt;&gt; out = module(torch.ones(2, 4))\n&gt;&gt;&gt; out\ntensor([[6., 6., 6., 6., 6.],\n        [6., 6., 6., 6., 6.]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre>"}]}